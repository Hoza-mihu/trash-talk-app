{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d7a48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# ðŸ“ˆ Model Evaluation\\n\",\n",
    "    \"Comprehensive model evaluation and analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from sklearn.metrics import classification_report, confusion_matrix\\n\",\n",
    "    \"import tensorflow as tf\\n\",\n",
    "    \"from tensorflow import keras\\n\",\n",
    "    \"\\n\",\n",
    "    \"sys.path.append('..')\\n\",\n",
    "    \"from utils.data_loader import create_data_generators\\n\",\n",
    "    \"from utils.visualization import plot_confusion_matrix, plot_sample_predictions\\n\",\n",
    "    \"\\n\",\n",
    "    \"sns.set_style('whitegrid')\\n\",\n",
    "    \"plt.rcParams['figure.figsize'] = (12, 8)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Load trained model\\n\",\n",
    "    \"MODEL_PATH = '../models/final/waste_classifier.h5'\\n\",\n",
    "    \"DATA_DIR = '../data/raw'\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = keras.models.load_model(MODEL_PATH)\\n\",\n",
    "    \"print('âœ… Model loaded successfully')\\n\",\n",
    "    \"model.summary()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Load Test Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Create test generator\\n\",\n",
    "    \"_, _, test_gen = create_data_generators(DATA_DIR)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f'Test samples: {test_gen.samples}')\\n\",\n",
    "    \"print(f'Classes: {list(test_gen.class_indices.keys())}')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Model Predictions\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Get predictions\\n\",\n",
    "    \"y_pred_probs = model.predict(test_gen, verbose=1)\\n\",\n",
    "    \"y_pred = np.argmax(y_pred_probs, axis=1)\\n\",\n",
    "    \"y_true = test_gen.classes\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get class names\\n\",\n",
    "    \"class_names = list(test_gen.class_indices.keys())\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f'Predictions shape: {y_pred_probs.shape}')\\n\",\n",
    "    \"print(f'Number of predictions: {len(y_pred)}')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Overall Metrics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Calculate overall accuracy\\n\",\n",
    "    \"accuracy = np.mean(y_pred == y_true)\\n\",\n",
    "    \"print(f'\\\\nðŸŽ¯ Overall Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Classification report\\n\",\n",
    "    \"report = classification_report(y_true, y_pred, target_names=class_names)\\n\",\n",
    "    \"print('\\\\nðŸ“Š Classification Report:')\\n\",\n",
    "    \"print(report)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Confusion Matrix\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Generate confusion matrix\\n\",\n",
    "    \"cm = confusion_matrix(y_true, y_pred)\\n\",\n",
    "    \"plot_confusion_matrix(cm, class_names)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Per-Class Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Calculate per-class metrics\\n\",\n",
    "    \"per_class_metrics = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, class_name in enumerate(class_names):\\n\",\n",
    "    \"    class_mask = y_true == i\\n\",\n",
    "    \"    class_acc = np.mean(y_pred[class_mask] == y_true[class_mask])\\n\",\n",
    "    \"    class_samples = np.sum(class_mask)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    per_class_metrics.append({\\n\",\n",
    "    \"        'Class': class_name,\\n\",\n",
    "    \"        'Accuracy': class_acc,\\n\",\n",
    "    \"        'Samples': class_samples,\\n\",\n",
    "    \"        'Correct': np.sum(y_pred[class_mask] == y_true[class_mask]),\\n\",\n",
    "    \"        'Incorrect': np.sum(y_pred[class_mask] != y_true[class_mask])\\n\",\n",
    "    \"    })\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_per_class = pd.DataFrame(per_class_metrics)\\n\",\n",
    "    \"df_per_class = df_per_class.sort_values('Accuracy', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print('\\\\nðŸ“Š Per-Class Performance:')\\n\",\n",
    "    \"print(df_per_class.to_string(index=False))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Plot per-class accuracy\\n\",\n",
    "    \"plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"bars = plt.bar(df_per_class['Class'], df_per_class['Accuracy'], color='steelblue', edgecolor='black')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Color bars based on accuracy\\n\",\n",
    "    \"for i, bar in enumerate(bars):\\n\",\n",
    "    \"    if df_per_class.iloc[i]['Accuracy'] >= 0.9:\\n\",\n",
    "    \"        bar.set_color('green')\\n\",\n",
    "    \"    elif df_per_class.iloc[i]['Accuracy'] >= 0.7:\\n\",\n",
    "    \"        bar.set_color('orange')\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        bar.set_color('red')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.axhline(y=accuracy, color='blue', linestyle='--', label=f'Overall Accuracy: {accuracy:.3f}')\\n\",\n",
    "    \"plt.title('Per-Class Accuracy', fontsize=16, fontweight='bold')\\n\",\n",
    "    \"plt.xlabel('Class', fontsize=12)\\n\",\n",
    "    \"plt.ylabel('Accuracy', fontsize=12)\\n\",\n",
    "    \"plt.ylim([0, 1])\\n\",\n",
    "    \"plt.xticks(rotation=45, ha='right')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.grid(axis='y', alpha=0.3)\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Confidence Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Analyze prediction confidence\\n\",\n",
    "    \"max_probs = np.max(y_pred_probs, axis=1)\\n\",\n",
    "    \"correct_mask = (y_pred == y_true)\\n\",\n",
    "    \"\\n\",\n",
    "    \"correct_confidence = max_probs[correct_mask]\\n\",\n",
    "    \"incorrect_confidence = max_probs[~correct_mask]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f'\\\\nðŸ“ˆ Confidence Analysis:')\\n\",\n",
    "    \"print(f'Correct predictions - Mean confidence: {correct_confidence.mean():.4f}')\\n\",\n",
    "    \"print(f'Incorrect predictions - Mean confidence: {incorrect_confidence.mean():.4f}')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot confidence distribution\\n\",\n",
    "    \"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"ax.hist(correct_confidence, bins=50, alpha=0.7, label='Correct', color='green', edgecolor='black')\\n\",\n",
    "    \"ax.hist(incorrect_confidence, bins=50, alpha=0.7, label='Incorrect', color='red', edgecolor='black')\\n\",\n",
    "    \"ax.set_xlabel('Confidence', fontsize=12)\\n\",\n",
    "    \"ax.set_ylabel('Frequency', fontsize=12)\\n\",\n",
    "    \"ax.set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax.legend()\\n\",\n",
    "    \"ax.grid(alpha=0.3)\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Error Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Find most common misclassifications\\n\",\n",
    "    \"errors = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i in range(len(class_names)):\\n\",\n",
    "    \"    for j in range(len(class_names)):\\n\",\n",
    "    \"        if i != j:\\n\",\n",
    "    \"            count = np.sum((y_true == i) & (y_pred == j))\\n\",\n",
    "    \"            if count > 0:\\n\",\n",
    "    \"                errors.append({\\n\",\n",
    "    \"                    'True Class': class_names[i],\\n\",\n",
    "    \"                    'Predicted Class': class_names[j],\\n\",\n",
    "    \"                    'Count': count\\n\",\n",
    "    \"                })\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_errors = pd.DataFrame(errors)\\n\",\n",
    "    \"df_errors = df_errors.sort_values('Count', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print('\\\\nâŒ Top 10 Misclassifications:')\\n\",\n",
    "    \"print(df_errors.head(10).to_string(index=False))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Sample Predictions\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Display sample predictions\\n\",\n",
    "    \"plot_sample_predictions(model, test_gen, num_samples=16)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 10. Model Summary Report\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Generate summary report\\n\",\n",
    "    \"report_data = {\\n\",\n",
    "    \"    'Metric': [\\n\",\n",
    "    \"        'Test Accuracy',\\n\",\n",
    "    \"        'Test Loss',\\n\",\n",
    "    \"        'Total Samples',\\n\",\n",
    "    \"        'Correct Predictions',\\n\",\n",
    "    \"        'Incorrect Predictions',\\n\",\n",
    "    \"        'Mean Confidence (Correct)',\\n\",\n",
    "    \"        'Mean Confidence (Incorrect)',\\n\",\n",
    "    \"        'Number of Classes'\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    'Value': [\\n\",\n",
    "    \"        f'{accuracy:.4f}',\\n\",\n",
    "    \"        f'{model.evaluate(test_gen, verbose=0)[0]:.4f}',\\n\",\n",
    "    \"        len(y_true),\\n\",\n",
    "    \"        np.sum(correct_mask),\\n\",\n",
    "    \"        np.sum(~correct_mask),\\n\",\n",
    "    \"        f'{correct_confidence.mean():.4f}',\\n\",\n",
    "    \"        f'{incorrect_confidence.mean():.4f}' if len(incorrect_confidence) > 0 else 'N/A',\\n\",\n",
    "    \"        len(class_names)\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_report = pd.DataFrame(report_data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print('\\\\n' + '='*50)\\n\",\n",
    "    \"print('ðŸ“Š MODEL EVALUATION SUMMARY')\\n\",\n",
    "    \"print('='*50)\\n\",\n",
    "    \"print(df_report.to_string(index=False))\\n\",\n",
    "    \"print('='*50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save report\\n\",\n",
    "    \"df_report.to_csv('../models/evaluation_summary.csv', index=False)\\n\",\n",
    "    \"print('\\\\nâœ… Evaluation summary saved to: models/evaluation_summary.csv')\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
